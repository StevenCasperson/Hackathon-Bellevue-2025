{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a28fc7c",
   "metadata": {},
   "source": [
    "# Human-in-the-Loop Data Analysis & ML Pipeline ü§ñüë•\n",
    "## Hackathon Project - October 4th, 2025\n",
    "\n",
    "**Theme:** Human-in-the-Loop (HITL)  \n",
    "**Tracks:** Healthcare & Sustainability  \n",
    "**Team:** TattleTale\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "This notebook demonstrates a comprehensive Human-in-the-Loop approach to data analysis and machine learning, where:\n",
    "- **AI provides insights and predictions**\n",
    "- **Humans validate, correct, and guide the process**\n",
    "- **Continuous feedback improves model performance**\n",
    "\n",
    "### HITL Applications:\n",
    "- **Healthcare:** Medical diagnosis assistance with doctor validation\n",
    "- **Sustainability:** Environmental monitoring with expert review\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a9392",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Let's start by importing all the essential libraries for our HITL data analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c55eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_curve, auc\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# For HITL interactivity\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Set style and suppress warnings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üéØ Ready for Human-in-the-Loop Data Analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4b215",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "**HITL Approach:** Let's load sample data and create an interactive exploration interface where humans can guide the analysis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4341aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets for both Healthcare and Sustainability tracks\n",
    "np.random.seed(42)\n",
    "\n",
    "def create_healthcare_dataset(n_samples=1000):\n",
    "    \"\"\"Create a sample healthcare dataset for medical diagnosis prediction\"\"\"\n",
    "    data = {\n",
    "        'age': np.random.normal(50, 15, n_samples).astype(int),\n",
    "        'bmi': np.random.normal(25, 5, n_samples),\n",
    "        'blood_pressure_systolic': np.random.normal(120, 20, n_samples),\n",
    "        'blood_pressure_diastolic': np.random.normal(80, 10, n_samples),\n",
    "        'cholesterol': np.random.normal(200, 40, n_samples),\n",
    "        'glucose_level': np.random.normal(100, 20, n_samples),\n",
    "        'family_history': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),\n",
    "        'smoking': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
    "        'exercise_hours_week': np.random.exponential(3, n_samples)\n",
    "    }\n",
    "    \n",
    "    # Create target variable (risk of heart disease)\n",
    "    risk_score = (\n",
    "        (data['age'] - 30) * 0.1 +\n",
    "        (data['bmi'] - 25) * 0.3 +\n",
    "        (data['blood_pressure_systolic'] - 120) * 0.05 +\n",
    "        (data['cholesterol'] - 200) * 0.02 +\n",
    "        data['family_history'] * 2 +\n",
    "        data['smoking'] * 1.5 +\n",
    "        np.random.normal(0, 1, n_samples)\n",
    "    )\n",
    "    \n",
    "    data['heart_disease_risk'] = (risk_score > np.percentile(risk_score, 70)).astype(int)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_sustainability_dataset(n_samples=1000):\n",
    "    \"\"\"Create a sample sustainability dataset for energy consumption prediction\"\"\"\n",
    "    data = {\n",
    "        'temperature': np.random.normal(20, 10, n_samples),\n",
    "        'humidity': np.random.normal(60, 15, n_samples),\n",
    "        'wind_speed': np.random.exponential(5, n_samples),\n",
    "        'solar_radiation': np.random.gamma(2, 2, n_samples) * 100,\n",
    "        'building_age': np.random.uniform(1, 50, n_samples),\n",
    "        'occupancy': np.random.poisson(10, n_samples),\n",
    "        'building_type': np.random.choice(['residential', 'commercial', 'industrial'], n_samples),\n",
    "        'insulation_rating': np.random.uniform(1, 10, n_samples),\n",
    "        'renewable_energy': np.random.choice([0, 1], n_samples, p=[0.6, 0.4])\n",
    "    }\n",
    "    \n",
    "    # Create target variable (energy consumption)\n",
    "    base_consumption = (\n",
    "        50 + data['building_age'] * 2 +\n",
    "        data['occupancy'] * 10 +\n",
    "        (data['temperature'] < 18) * 20 +\n",
    "        (data['temperature'] > 25) * 30 +\n",
    "        (data['building_type'] == 'commercial') * 50 +\n",
    "        (data['building_type'] == 'industrial') * 100 +\n",
    "        (10 - data['insulation_rating']) * 5 +\n",
    "        data['renewable_energy'] * -30 +\n",
    "        np.random.normal(0, 20, n_samples)\n",
    "    )\n",
    "    \n",
    "    data['energy_consumption'] = np.maximum(base_consumption, 10)  # Minimum consumption\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Interactive dataset selection\n",
    "@interact\n",
    "def select_dataset(dataset_type=['Healthcare', 'Sustainability']):\n",
    "    global df, target_column, problem_type\n",
    "    \n",
    "    if dataset_type == 'Healthcare':\n",
    "        df = create_healthcare_dataset()\n",
    "        target_column = 'heart_disease_risk'\n",
    "        problem_type = 'classification'\n",
    "        print(\"üè• Healthcare Dataset Loaded - Heart Disease Risk Prediction\")\n",
    "        print(\"üìä Classification Problem\")\n",
    "    else:\n",
    "        df = create_sustainability_dataset()\n",
    "        target_column = 'energy_consumption'\n",
    "        problem_type = 'regression'\n",
    "        print(\"üå± Sustainability Dataset Loaded - Energy Consumption Prediction\")\n",
    "        print(\"üìä Regression Problem\")\n",
    "    \n",
    "    print(f\"\\nüìã Dataset Shape: {df.shape}\")\n",
    "    print(f\"üéØ Target Variable: {target_column}\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    display(df.head())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d2ae4",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing\n",
    "**HITL Approach:** Interactive data quality assessment where humans can review and approve AI-suggested cleaning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HITLDataCleaner:\n",
    "    \"\"\"Human-in-the-Loop Data Cleaning Assistant\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe.copy()\n",
    "        self.original_df = dataframe.copy()\n",
    "        self.cleaning_log = []\n",
    "    \n",
    "    def assess_data_quality(self):\n",
    "        \"\"\"AI analyzes data quality and suggests cleaning operations\"\"\"\n",
    "        print(\"ü§ñ AI Assessment: Analyzing data quality...\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Missing values\n",
    "        missing_data = self.df.isnull().sum()\n",
    "        if missing_data.sum() > 0:\n",
    "            print(f\"‚ö†Ô∏è  Missing values detected:\")\n",
    "            for col, count in missing_data[missing_data > 0].items():\n",
    "                print(f\"   - {col}: {count} missing ({count/len(self.df)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"‚úÖ No missing values detected\")\n",
    "        \n",
    "        # Duplicates\n",
    "        duplicates = self.df.duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            print(f\"‚ö†Ô∏è  {duplicates} duplicate rows detected\")\n",
    "        else:\n",
    "            print(\"‚úÖ No duplicate rows detected\")\n",
    "        \n",
    "        # Outliers (for numerical columns)\n",
    "        numerical_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        outlier_cols = []\n",
    "        for col in numerical_cols:\n",
    "            Q1 = self.df[col].quantile(0.25)\n",
    "            Q3 = self.df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = ((self.df[col] < Q1 - 1.5*IQR) | (self.df[col] > Q3 + 1.5*IQR)).sum()\n",
    "            if outliers > 0:\n",
    "                outlier_cols.append((col, outliers))\n",
    "        \n",
    "        if outlier_cols:\n",
    "            print(f\"‚ö†Ô∏è  Potential outliers detected:\")\n",
    "            for col, count in outlier_cols:\n",
    "                print(f\"   - {col}: {count} outliers ({count/len(self.df)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"‚úÖ No significant outliers detected\")\n",
    "        \n",
    "        return missing_data, duplicates, outlier_cols\n",
    "    \n",
    "    def interactive_cleaning(self):\n",
    "        \"\"\"Interactive interface for human review of AI suggestions\"\"\"\n",
    "        missing_data, duplicates, outlier_cols = self.assess_data_quality()\n",
    "        \n",
    "        # Handle missing values\n",
    "        if missing_data.sum() > 0:\n",
    "            print(\"\\nüßπ Cleaning Suggestions:\")\n",
    "            for col in missing_data[missing_data > 0].index:\n",
    "                if self.df[col].dtype in ['object']:\n",
    "                    suggestion = f\"Fill '{col}' missing values with mode: '{self.df[col].mode()[0]}'\"\n",
    "                else:\n",
    "                    suggestion = f\"Fill '{col}' missing values with median: {self.df[col].median():.2f}\"\n",
    "                \n",
    "                print(f\"üí° AI Suggests: {suggestion}\")\n",
    "                # In a real HITL system, this would be an interactive widget\n",
    "                # For demo purposes, we'll auto-apply with logging\n",
    "                self.apply_cleaning_suggestion(col, missing_data[col])\n",
    "        \n",
    "        # Handle duplicates\n",
    "        if duplicates > 0:\n",
    "            print(f\"\\nüí° AI Suggests: Remove {duplicates} duplicate rows\")\n",
    "            self.df = self.df.drop_duplicates()\n",
    "            self.cleaning_log.append(f\"Removed {duplicates} duplicate rows\")\n",
    "            print(f\"‚úÖ Applied: Removed duplicates\")\n",
    "        \n",
    "        print(f\"\\nüìä Cleaned dataset shape: {self.df.shape}\")\n",
    "        return self.df\n",
    "    \n",
    "    def apply_cleaning_suggestion(self, column, missing_count):\n",
    "        \"\"\"Apply AI cleaning suggestion with human approval simulation\"\"\"\n",
    "        if self.df[column].dtype in ['object']:\n",
    "            fill_value = self.df[column].mode()[0]\n",
    "            self.df[column].fillna(fill_value, inplace=True)\n",
    "            action = f\"Filled {missing_count} missing values in '{column}' with mode: '{fill_value}'\"\n",
    "        else:\n",
    "            fill_value = self.df[column].median()\n",
    "            self.df[column].fillna(fill_value, inplace=True)\n",
    "            action = f\"Filled {missing_count} missing values in '{column}' with median: {fill_value:.2f}\"\n",
    "        \n",
    "        self.cleaning_log.append(action)\n",
    "        print(f\"‚úÖ Applied: {action}\")\n",
    "\n",
    "# Apply HITL cleaning\n",
    "if 'df' in globals():\n",
    "    cleaner = HITLDataCleaner(df)\n",
    "    df_cleaned = cleaner.interactive_cleaning()\n",
    "    \n",
    "    print(\"\\nüìã Cleaning Log:\")\n",
    "    for i, action in enumerate(cleaner.cleaning_log, 1):\n",
    "        print(f\"{i}. {action}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please run the dataset selection cell first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f41fd5",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "**HITL Approach:** AI generates multiple visualization options, human selects the most insightful ones for deeper analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HITLExplorer:\n",
    "    \"\"\"Human-in-the-Loop Exploratory Data Analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, target_column):\n",
    "        self.df = dataframe\n",
    "        self.target = target_column\n",
    "        self.insights = []\n",
    "    \n",
    "    def generate_ai_insights(self):\n",
    "        \"\"\"AI generates automatic insights about the data\"\"\"\n",
    "        print(\"ü§ñ AI-Generated Insights:\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(f\"üìä Dataset contains {len(self.df)} samples with {len(self.df.columns)} features\")\n",
    "        \n",
    "        # Target distribution\n",
    "        if self.df[self.target].dtype in ['int64', 'float64'] and len(self.df[self.target].unique()) <= 10:\n",
    "            # Classification\n",
    "            target_dist = self.df[self.target].value_counts()\n",
    "            print(f\"üéØ Target distribution: {dict(target_dist)}\")\n",
    "            \n",
    "            # Check for class imbalance\n",
    "            imbalance_ratio = target_dist.max() / target_dist.min()\n",
    "            if imbalance_ratio > 2:\n",
    "                print(f\"‚ö†Ô∏è  Class imbalance detected (ratio: {imbalance_ratio:.1f})\")\n",
    "                self.insights.append(\"Consider class balancing techniques\")\n",
    "        else:\n",
    "            # Regression\n",
    "            print(f\"üéØ Target range: {self.df[self.target].min():.2f} to {self.df[self.target].max():.2f}\")\n",
    "            print(f\"üéØ Target mean: {self.df[self.target].mean():.2f} ¬± {self.df[self.target].std():.2f}\")\n",
    "        \n",
    "        # Feature correlations\n",
    "        numerical_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numerical_cols) > 1:\n",
    "            corr_with_target = self.df[numerical_cols].corr()[self.target].abs().sort_values(ascending=False)\n",
    "            top_corr = corr_with_target.drop(self.target).head(3)\n",
    "            print(f\"üîó Top correlated features with target:\")\n",
    "            for feature, corr in top_corr.items():\n",
    "                print(f\"   - {feature}: {corr:.3f}\")\n",
    "        \n",
    "        return self.insights\n",
    "    \n",
    "    def interactive_visualization(self):\n",
    "        \"\"\"Interactive visualization selection\"\"\"\n",
    "        print(\"\\nüìà Available Visualizations:\")\n",
    "        \n",
    "        # Create multiple visualization options\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('AI-Suggested Visualizations for Human Review', fontsize=16)\n",
    "        \n",
    "        # Plot 1: Target distribution\n",
    "        if self.df[self.target].dtype in ['int64', 'float64'] and len(self.df[self.target].unique()) <= 10:\n",
    "            self.df[self.target].value_counts().plot(kind='bar', ax=axes[0,0])\n",
    "            axes[0,0].set_title('Target Distribution')\n",
    "        else:\n",
    "            self.df[self.target].hist(bins=30, ax=axes[0,0])\n",
    "            axes[0,0].set_title('Target Distribution')\n",
    "        \n",
    "        # Plot 2: Correlation heatmap\n",
    "        numerical_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numerical_cols) > 2:\n",
    "            corr_matrix = self.df[numerical_cols].corr()\n",
    "            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0,1])\n",
    "            axes[0,1].set_title('Feature Correlations')\n",
    "        \n",
    "        # Plot 3: Feature importance preview\n",
    "        if len(numerical_cols) > 1:\n",
    "            # Quick feature importance using correlation\n",
    "            feature_importance = abs(self.df[numerical_cols].corr()[self.target]).drop(self.target).sort_values(ascending=True)\n",
    "            feature_importance.tail(5).plot(kind='barh', ax=axes[1,0])\n",
    "            axes[1,0].set_title('Top 5 Feature Correlations')\n",
    "        \n",
    "        # Plot 4: Data distribution overview\n",
    "        if len(numerical_cols) > 2:\n",
    "            # Box plot of top features\n",
    "            top_features = abs(self.df[numerical_cols].corr()[self.target]).drop(self.target).nlargest(3).index\n",
    "            self.df[top_features].boxplot(ax=axes[1,1])\n",
    "            axes[1,1].set_title('Top Features Distribution')\n",
    "            axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Insight summary\n",
    "        print(\"\\nüí° Human Review Points:\")\n",
    "        print(\"1. Does the target distribution look reasonable?\")\n",
    "        print(\"2. Are there any unexpected correlations?\")\n",
    "        print(\"3. Do you see any patterns that need investigation?\")\n",
    "        print(\"4. Should we focus on specific features?\")\n",
    "\n",
    "# Run HITL EDA\n",
    "if 'df_cleaned' in globals():\n",
    "    explorer = HITLExplorer(df_cleaned, target_column)\n",
    "    insights = explorer.generate_ai_insights()\n",
    "    explorer.interactive_visualization()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please run the data cleaning step first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274802be",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "**HITL Approach:** AI suggests feature transformations, humans validate and approve the most promising ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HITLFeatureEngineer:\n",
    "    \"\"\"Human-in-the-Loop Feature Engineering Assistant\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, target_column, problem_type):\n",
    "        self.df = dataframe.copy()\n",
    "        self.target = target_column\n",
    "        self.problem_type = problem_type\n",
    "        self.feature_suggestions = []\n",
    "        self.applied_features = []\n",
    "    \n",
    "    def suggest_features(self):\n",
    "        \"\"\"AI suggests potential feature engineering operations\"\"\"\n",
    "        print(\"ü§ñ AI Feature Engineering Suggestions:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        numerical_cols = self.df.select_dtypes(include=[np.number]).columns.drop(self.target)\n",
    "        categorical_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        suggestions = []\n",
    "        \n",
    "        # 1. Polynomial features for numerical columns\n",
    "        if len(numerical_cols) >= 2:\n",
    "            suggestions.append({\n",
    "                'type': 'polynomial',\n",
    "                'description': f'Create interaction terms between top numerical features',\n",
    "                'rationale': 'Capture non-linear relationships'\n",
    "            })\n",
    "        \n",
    "        # 2. Binning for continuous variables\n",
    "        for col in numerical_cols:\n",
    "            if self.df[col].nunique() > 20:\n",
    "                suggestions.append({\n",
    "                    'type': 'binning',\n",
    "                    'column': col,\n",
    "                    'description': f'Create categorical bins for {col}',\n",
    "                    'rationale': 'Convert continuous to categorical for easier interpretation'\n",
    "                })\n",
    "        \n",
    "        # 3. Encoding categorical variables\n",
    "        if len(categorical_cols) > 0:\n",
    "            suggestions.append({\n",
    "                'type': 'encoding',\n",
    "                'description': f'Encode categorical variables: {list(categorical_cols)}',\n",
    "                'rationale': 'Convert categorical data for ML algorithms'\n",
    "            })\n",
    "        \n",
    "        # 4. Feature scaling\n",
    "        suggestions.append({\n",
    "            'type': 'scaling',\n",
    "            'description': 'Standardize numerical features',\n",
    "            'rationale': 'Ensure all features have similar scales'\n",
    "        })\n",
    "        \n",
    "        # 5. Domain-specific features\n",
    "        if 'age' in numerical_cols:\n",
    "            suggestions.append({\n",
    "                'type': 'domain_specific',\n",
    "                'description': 'Create age groups (young, middle-aged, senior)',\n",
    "                'rationale': 'Age categories may be more predictive than raw age'\n",
    "            })\n",
    "        \n",
    "        self.feature_suggestions = suggestions\n",
    "        \n",
    "        for i, suggestion in enumerate(suggestions, 1):\n",
    "            print(f\"{i}. {suggestion['description']}\")\n",
    "            print(f\"   üí° Rationale: {suggestion['rationale']}\")\n",
    "            print()\n",
    "        \n",
    "        return suggestions\n",
    "    \n",
    "    def apply_feature_engineering(self, selected_suggestions=None):\n",
    "        \"\"\"Apply selected feature engineering suggestions\"\"\"\n",
    "        if selected_suggestions is None:\n",
    "            # Auto-apply all suggestions for demo\n",
    "            selected_suggestions = list(range(len(self.feature_suggestions)))\n",
    "        \n",
    "        df_engineered = self.df.copy()\n",
    "        \n",
    "        for idx in selected_suggestions:\n",
    "            if idx >= len(self.feature_suggestions):\n",
    "                continue\n",
    "                \n",
    "            suggestion = self.feature_suggestions[idx]\n",
    "            \n",
    "            if suggestion['type'] == 'polynomial':\n",
    "                # Create interaction features for top 3 correlated features\n",
    "                numerical_cols = df_engineered.select_dtypes(include=[np.number]).columns.drop(self.target)\n",
    "                if len(numerical_cols) >= 2:\n",
    "                    corr_with_target = df_engineered[numerical_cols].corrwith(df_engineered[self.target]).abs()\n",
    "                    top_features = corr_with_target.nlargest(2).index.tolist()\n",
    "                    \n",
    "                    for i in range(len(top_features)):\n",
    "                        for j in range(i+1, len(top_features)):\n",
    "                            feature_name = f\"{top_features[i]}_{top_features[j]}_interaction\"\n",
    "                            df_engineered[feature_name] = df_engineered[top_features[i]] * df_engineered[top_features[j]]\n",
    "                            self.applied_features.append(feature_name)\n",
    "                    \n",
    "                    print(f\"‚úÖ Created interaction features: {self.applied_features[-len(top_features):]}\")\n",
    "            \n",
    "            elif suggestion['type'] == 'binning' and 'column' in suggestion:\n",
    "                col = suggestion['column']\n",
    "                if col in df_engineered.columns:\n",
    "                    # Create quantile-based bins\n",
    "                    df_engineered[f\"{col}_binned\"] = pd.qcut(df_engineered[col], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "                    self.applied_features.append(f\"{col}_binned\")\n",
    "                    print(f\"‚úÖ Created bins for {col}\")\n",
    "            \n",
    "            elif suggestion['type'] == 'encoding':\n",
    "                # One-hot encode categorical variables\n",
    "                categorical_cols = df_engineered.select_dtypes(include=['object']).columns\n",
    "                for col in categorical_cols:\n",
    "                    if df_engineered[col].nunique() <= 10:  # Only encode if not too many categories\n",
    "                        dummies = pd.get_dummies(df_engineered[col], prefix=col)\n",
    "                        df_engineered = pd.concat([df_engineered, dummies], axis=1)\n",
    "                        df_engineered.drop(col, axis=1, inplace=True)\n",
    "                        self.applied_features.extend(dummies.columns.tolist())\n",
    "                \n",
    "                print(f\"‚úÖ One-hot encoded categorical variables\")\n",
    "            \n",
    "            elif suggestion['type'] == 'scaling':\n",
    "                # Note: We'll store this for later application during model training\n",
    "                print(f\"‚úÖ Scaling will be applied during model training\")\n",
    "            \n",
    "            elif suggestion['type'] == 'domain_specific':\n",
    "                if 'age' in df_engineered.columns:\n",
    "                    df_engineered['age_group'] = pd.cut(df_engineered['age'], \n",
    "                                                       bins=[0, 30, 50, 70, 100], \n",
    "                                                       labels=['Young', 'Middle', 'Senior', 'Elderly'])\n",
    "                    self.applied_features.append('age_group')\n",
    "                    print(f\"‚úÖ Created age groups\")\n",
    "        \n",
    "        print(f\"\\nüìä Original features: {self.df.shape[1]}\")\n",
    "        print(f\"üìä Engineered features: {df_engineered.shape[1]}\")\n",
    "        print(f\"üìä New features created: {len(self.applied_features)}\")\n",
    "        \n",
    "        return df_engineered\n",
    "    \n",
    "    def feature_importance_preview(self, df_engineered):\n",
    "        \"\"\"Preview feature importance of engineered features\"\"\"\n",
    "        numerical_cols = df_engineered.select_dtypes(include=[np.number]).columns\n",
    "        if self.target in numerical_cols and len(numerical_cols) > 1:\n",
    "            correlations = df_engineered[numerical_cols].corrwith(df_engineered[self.target]).abs().sort_values(ascending=False)\n",
    "            \n",
    "            print(\"\\nüîó Feature Correlations with Target (Top 10):\")\n",
    "            print(\"-\" * 40)\n",
    "            for feature, corr in correlations.drop(self.target).head(10).items():\n",
    "                is_new = \"üÜï\" if feature in self.applied_features else \"   \"\n",
    "                print(f\"{is_new} {feature}: {corr:.3f}\")\n",
    "\n",
    "# Apply feature engineering\n",
    "if 'df_cleaned' in globals():\n",
    "    engineer = HITLFeatureEngineer(df_cleaned, target_column, problem_type)\n",
    "    suggestions = engineer.suggest_features()\n",
    "    \n",
    "    print(\"\\nüß† Human Decision: Apply all AI suggestions? (Auto-applying for demo)\")\n",
    "    df_engineered = engineer.apply_feature_engineering()\n",
    "    engineer.feature_importance_preview(df_engineered)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please run the previous steps first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae702b6b",
   "metadata": {},
   "source": [
    "## 6. Data Splitting & Model Selection\n",
    "**HITL Approach:** AI recommends optimal train/test split and suggests appropriate algorithms based on data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ab994",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HITLModelSelector:\n",
    "    \"\"\"Human-in-the-Loop Model Selection Assistant\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, target_column, problem_type):\n",
    "        self.df = dataframe\n",
    "        self.target = target_column\n",
    "        self.problem_type = problem_type\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "    \n",
    "    def analyze_data_characteristics(self):\n",
    "        \"\"\"AI analyzes data to recommend appropriate models\"\"\"\n",
    "        print(\"ü§ñ AI Data Analysis for Model Recommendation:\")\n",
    "        print(\"=\"*55)\n",
    "        \n",
    "        n_samples, n_features = self.df.shape\n",
    "        n_features -= 1  # Subtract target column\n",
    "        \n",
    "        print(f\"üìä Dataset size: {n_samples} samples, {n_features} features\")\n",
    "        print(f\"üìä Problem type: {self.problem_type}\")\n",
    "        \n",
    "        # Data characteristics\n",
    "        characteristics = {\n",
    "            'small_dataset': n_samples < 1000,\n",
    "            'high_dimensional': n_features > n_samples * 0.1,\n",
    "            'binary_classification': self.problem_type == 'classification' and len(self.df[self.target].unique()) == 2,\n",
    "            'multiclass': self.problem_type == 'classification' and len(self.df[self.target].unique()) > 2,\n",
    "            'regression': self.problem_type == 'regression'\n",
    "        }\n",
    "        \n",
    "        print(f\"üìä Small dataset: {characteristics['small_dataset']}\")\n",
    "        print(f\"üìä High dimensional: {characteristics['high_dimensional']}\")\n",
    "        \n",
    "        return characteristics\n",
    "    \n",
    "    def recommend_models(self, characteristics):\n",
    "        \"\"\"AI recommends models based on data characteristics\"\"\"\n",
    "        print(\"\\nüí° AI Model Recommendations:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        if self.problem_type == 'classification':\n",
    "            recommendations.extend([\n",
    "                {\n",
    "                    'name': 'Random Forest',\n",
    "                    'model': RandomForestClassifier(random_state=42),\n",
    "                    'rationale': 'Robust, handles mixed data types, feature importance',\n",
    "                    'priority': 'High'\n",
    "                },\n",
    "                {\n",
    "                    'name': 'Logistic Regression',\n",
    "                    'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "                    'rationale': 'Interpretable, good baseline, fast training',\n",
    "                    'priority': 'High'\n",
    "                }\n",
    "            ])\n",
    "            \n",
    "            if not characteristics['small_dataset']:\n",
    "                recommendations.append({\n",
    "                    'name': 'Support Vector Machine',\n",
    "                    'model': SVC(random_state=42, probability=True),\n",
    "                    'rationale': 'Good for complex decision boundaries',\n",
    "                    'priority': 'Medium'\n",
    "                })\n",
    "        \n",
    "        else:  # regression\n",
    "            recommendations.extend([\n",
    "                {\n",
    "                    'name': 'Random Forest',\n",
    "                    'model': RandomForestRegressor(random_state=42),\n",
    "                    'rationale': 'Robust, non-linear relationships, feature importance',\n",
    "                    'priority': 'High'\n",
    "                },\n",
    "                {\n",
    "                    'name': 'Linear Regression',\n",
    "                    'model': LinearRegression(),\n",
    "                    'rationale': 'Interpretable, fast, good baseline',\n",
    "                    'priority': 'High'\n",
    "                }\n",
    "            ])\n",
    "            \n",
    "            if not characteristics['small_dataset']:\n",
    "                recommendations.append({\n",
    "                    'name': 'Support Vector Regression',\n",
    "                    'model': SVR(),\n",
    "                    'rationale': 'Good for non-linear relationships',\n",
    "                    'priority': 'Medium'\n",
    "                })\n",
    "        \n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"{i}. {rec['name']} [{rec['priority']} Priority]\")\n",
    "            print(f\"   üí° {rec['rationale']}\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare data for modeling with human oversight\"\"\"\n",
    "        print(\"\\nüîß Data Preparation:\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = self.df.drop(columns=[self.target])\n",
    "        y = self.df[self.target]\n",
    "        \n",
    "        # Handle categorical variables\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        if len(categorical_cols) > 0:\n",
    "            print(f\"üîÑ Encoding {len(categorical_cols)} categorical columns\")\n",
    "            # Simple label encoding for demo (in practice, might use one-hot)\n",
    "            le = LabelEncoder()\n",
    "            for col in categorical_cols:\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "        \n",
    "        # Feature scaling\n",
    "        print(\"üìè Applying feature scaling\")\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Train-test split\n",
    "        test_size = 0.2 if len(self.df) > 100 else 0.3\n",
    "        print(f\"üîÄ Splitting data: {int((1-test_size)*100)}% train, {int(test_size*100)}% test\")\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=test_size, random_state=42, stratify=y if self.problem_type == 'classification' else None\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Training set: {X_train.shape[0]} samples\")\n",
    "        print(f\"‚úÖ Testing set: {X_test.shape[0]} samples\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, scaler\n",
    "    \n",
    "    def train_and_evaluate_models(self, X_train, X_test, y_train, y_test, model_recommendations):\n",
    "        \"\"\"Train multiple models and let human review results\"\"\"\n",
    "        print(\"\\nüöÄ Training Models:\")\n",
    "        print(\"=\"*25)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for rec in model_recommendations:\n",
    "            model_name = rec['name']\n",
    "            model = rec['model']\n",
    "            \n",
    "            print(f\"\\nüîÑ Training {model_name}...\")\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if self.problem_type == 'classification':\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, average='weighted')\n",
    "                recall = recall_score(y_test, y_pred, average='weighted')\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                \n",
    "                results[model_name] = {\n",
    "                    'model': model,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1_score': f1,\n",
    "                    'predictions': y_pred\n",
    "                }\n",
    "                \n",
    "                print(f\"   üìä Accuracy: {accuracy:.3f}\")\n",
    "                print(f\"   üìä Precision: {precision:.3f}\")\n",
    "                print(f\"   üìä Recall: {recall:.3f}\")\n",
    "                print(f\"   üìä F1-Score: {f1:.3f}\")\n",
    "                \n",
    "            else:  # regression\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                rmse = np.sqrt(mse)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                \n",
    "                results[model_name] = {\n",
    "                    'model': model,\n",
    "                    'mse': mse,\n",
    "                    'rmse': rmse,\n",
    "                    'r2_score': r2,\n",
    "                    'predictions': y_pred\n",
    "                }\n",
    "                \n",
    "                print(f\"   üìä RMSE: {rmse:.3f}\")\n",
    "                print(f\"   üìä R¬≤ Score: {r2:.3f}\")\n",
    "                print(f\"   üìä MSE: {mse:.3f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Execute model selection pipeline\n",
    "if 'df_engineered' in globals():\n",
    "    selector = HITLModelSelector(df_engineered, target_column, problem_type)\n",
    "    \n",
    "    # Step 1: Analyze data characteristics\n",
    "    characteristics = selector.analyze_data_characteristics()\n",
    "    \n",
    "    # Step 2: Get AI recommendations\n",
    "    model_recommendations = selector.recommend_models(characteristics)\n",
    "    \n",
    "    # Step 3: Prepare data\n",
    "    X_train, X_test, y_train, y_test, scaler = selector.prepare_data()\n",
    "    \n",
    "    # Step 4: Train and evaluate models\n",
    "    model_results = selector.train_and_evaluate_models(X_train, X_test, y_train, y_test, model_recommendations)\n",
    "    \n",
    "    print(\"\\nüèÜ Human Review: Which model performs best for your use case?\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please run the feature engineering step first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c6612",
   "metadata": {},
   "source": [
    "## 7. Human-in-the-Loop Model Validation\n",
    "**HITL Approach:** Interactive model evaluation where humans can validate predictions, identify edge cases, and provide feedback for model improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e9d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HITLValidator:\n",
    "    \"\"\"Human-in-the-Loop Model Validation System\"\"\"\n",
    "    \n",
    "    def __init__(self, models_dict, X_test, y_test, problem_type):\n",
    "        self.models = models_dict\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.problem_type = problem_type\n",
    "        self.validation_feedback = []\n",
    "        self.edge_cases = []\n",
    "    \n",
    "    def interactive_prediction_review(self, model_name, n_samples=10):\n",
    "        \"\"\"Human reviews individual predictions\"\"\"\n",
    "        print(f\"üîç Human Validation: Reviewing {model_name} Predictions\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if model_name not in self.models:\n",
    "            print(\"‚ùå Model not found!\")\n",
    "            return\n",
    "        \n",
    "        model_info = self.models[model_name]\n",
    "        model = model_info['model']\n",
    "        predictions = model_info['predictions']\n",
    "        \n",
    "        # Get confidence scores if available\n",
    "        if hasattr(model, 'predict_proba') and self.problem_type == 'classification':\n",
    "            probabilities = model.predict_proba(self.X_test)\n",
    "            confidence_scores = np.max(probabilities, axis=1)\n",
    "        else:\n",
    "            confidence_scores = np.ones(len(predictions))  # Default confidence\n",
    "        \n",
    "        # Sample cases for review (focus on low confidence and edge cases)\n",
    "        review_indices = self._select_review_cases(predictions, confidence_scores, n_samples)\n",
    "        \n",
    "        print(f\"üìã Reviewing {len(review_indices)} cases (selected by AI for human review):\\\")\\n\"\n",
    "        \n",
    "        correct_predictions = 0\n",
    "        for i, idx in enumerate(review_indices, 1):\n",
    "            actual = self.y_test.iloc[idx]\n",
    "            predicted = predictions[idx]\n",
    "            confidence = confidence_scores[idx]\n",
    "            \n",
    "            print(f\"Case {i}/{len(review_indices)}:\")\n",
    "            print(f\"   üéØ Actual: {actual}\")\n",
    "            print(f\"   ü§ñ Predicted: {predicted}\")\n",
    "            print(f\"   üìä Confidence: {confidence:.3f}\")\n",
    "            \n",
    "            # Show some feature values for context\n",
    "            feature_sample = self.X_test.iloc[idx].head(3)\n",
    "            print(f\"   üìà Key features: {dict(feature_sample)}\")\n",
    "            \n",
    "            # Human validation (simulated)\n",
    "            is_correct = (actual == predicted) if self.problem_type == 'classification' else abs(actual - predicted) < abs(actual * 0.1)\n",
    "            human_agrees = np.random.choice([True, False], p=[0.9 if is_correct else 0.3, 0.1 if is_correct else 0.7])\n",
    "            \n",
    "            if human_agrees:\n",
    "                print(f\"   ‚úÖ Human validation: CORRECT\")\n",
    "                correct_predictions += 1\n",
    "            else:\n",
    "                print(f\"   ‚ùå Human validation: INCORRECT\")\n",
    "                self.edge_cases.append({\n",
    "                    'index': idx,\n",
    "                    'actual': actual,\n",
    "                    'predicted': predicted,\n",
    "                    'confidence': confidence,\n",
    "                    'features': dict(self.X_test.iloc[idx])\n",
    "                })\n",
    "            \n",
    "            print(f\"   üí¨ AI Learning: {'Reinforcing prediction' if human_agrees else 'Flagging for improvement'}\")\n",
    "            print()\n",
    "        \n",
    "        human_accuracy = correct_predictions / len(review_indices)\n",
    "        print(f\"üèÜ Human-Validated Accuracy: {human_accuracy:.3f}\")\n",
    "        print(f\"üö® Edge cases identified: {len([case for case in self.edge_cases if case['index'] in review_indices])}\")\n",
    "        \n",
    "        return human_accuracy\n",
    "    \n",
    "    def _select_review_cases(self, predictions, confidence_scores, n_samples):\n",
    "        \"\"\"AI selects most important cases for human review\"\"\"\n",
    "        # Prioritize low confidence predictions\n",
    "        low_confidence_indices = np.where(confidence_scores < 0.7)[0]\n",
    "        \n",
    "        # Add some random samples\n",
    "        random_indices = np.random.choice(len(predictions), size=min(n_samples//2, len(predictions)), replace=False)\n",
    "        \n",
    "        # Combine and select unique indices\n",
    "        review_indices = np.unique(np.concatenate([\n",
    "            low_confidence_indices[:n_samples//2],\n",
    "            random_indices\n",
    "        ]))[:n_samples]\n",
    "        \n",
    "        return review_indices\n",
    "    \n",
    "    def analyze_edge_cases(self):\n",
    "        \"\"\"Analyze patterns in edge cases for model improvement\"\"\"\n",
    "        if not self.edge_cases:\n",
    "            print(\"üéâ No edge cases found! Model performing well.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üî¨ Edge Case Analysis ({len(self.edge_cases)} cases):\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Analyze confidence distribution of edge cases\n",
    "        edge_confidences = [case['confidence'] for case in self.edge_cases]\n",
    "        avg_confidence = np.mean(edge_confidences)\n",
    "        \n",
    "        print(f\"üìä Average confidence in edge cases: {avg_confidence:.3f}\")\n",
    "        \n",
    "        if avg_confidence < 0.5:\n",
    "            print(\"üí° Insight: Model is appropriately uncertain about difficult cases\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Insight: Model overconfident in incorrect predictions\")\n",
    "        \n",
    "        # Feature analysis for edge cases\n",
    "        if len(self.edge_cases) > 2:\n",
    "            print(\"\\\\nüîç Common patterns in edge cases:\")\n",
    "            \n",
    "            # This is a simplified analysis - in practice, you'd do more sophisticated pattern detection\n",
    "            edge_case_features = pd.DataFrame([case['features'] for case in self.edge_cases])\n",
    "            \n",
    "            for col in edge_case_features.select_dtypes(include=[np.number]).columns[:3]:\n",
    "                mean_val = edge_case_features[col].mean()\n",
    "                overall_mean = self.X_test[col].mean()\n",
    "                diff = abs(mean_val - overall_mean) / overall_mean if overall_mean != 0 else 0\n",
    "                \n",
    "                if diff > 0.2:\n",
    "                    print(f\"   üìà {col}: Edge cases have different distribution (diff: {diff:.2f})\")\n",
    "        \n",
    "        return self.edge_cases\n",
    "    \n",
    "    def generate_improvement_suggestions(self):\n",
    "        \"\"\"AI generates suggestions for model improvement based on human feedback\"\"\"\n",
    "        print(\"\\\\nüöÄ AI Improvement Suggestions:\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        suggestions = []\n",
    "        \n",
    "        if len(self.edge_cases) > len(self.y_test) * 0.1:\n",
    "            suggestions.append(\"Consider collecting more training data for underrepresented cases\")\n",
    "        \n",
    "        if any(case['confidence'] > 0.8 for case in self.edge_cases):\n",
    "            suggestions.append(\"Implement confidence calibration to improve prediction reliability\")\n",
    "        \n",
    "        if len(self.edge_cases) > 0:\n",
    "            suggestions.append(\"Use active learning to focus on similar challenging cases\")\n",
    "            suggestions.append(\"Consider ensemble methods to improve robustness\")\n",
    "        \n",
    "        suggestions.append(\"Implement continuous learning pipeline with human feedback\")\n",
    "        suggestions.append(\"Set up monitoring for model drift in production\")\n",
    "        \n",
    "        for i, suggestion in enumerate(suggestions, 1):\n",
    "            print(f\"{i}. {suggestion}\")\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "# Run Human-in-the-Loop Validation\n",
    "if 'model_results' in globals() and len(model_results) > 0:\n",
    "    # Select best performing model for detailed validation\n",
    "    if problem_type == 'classification':\n",
    "        best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['f1_score'])\n",
    "        print(f\"üèÜ Best Model Selected: {best_model_name} (F1-Score: {model_results[best_model_name]['f1_score']:.3f})\")\n",
    "    else:\n",
    "        best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['r2_score'])\n",
    "        print(f\"üèÜ Best Model Selected: {best_model_name} (R¬≤: {model_results[best_model_name]['r2_score']:.3f})\")\n",
    "    \n",
    "    # Initialize validator\n",
    "    validator = HITLValidator(model_results, X_test, y_test, problem_type)\n",
    "    \n",
    "    # Human validation of predictions\n",
    "    human_accuracy = validator.interactive_prediction_review(best_model_name, n_samples=8)\n",
    "    \n",
    "    # Analyze edge cases\n",
    "    edge_cases = validator.analyze_edge_cases()\n",
    "    \n",
    "    # Generate improvement suggestions\n",
    "    suggestions = validator.generate_improvement_suggestions()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please run the model training step first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb6f2e2",
   "metadata": {},
   "source": [
    "## 8. Final Results & HITL Dashboard\n",
    "**HITL Summary:** Comprehensive dashboard showing the collaboration between human expertise and AI capabilities throughout the analysis pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d78e74",
   "metadata": {},
   "source": [
    "## 9. Energy Optimization with Real Data\n",
    "**HITL Approach:** Use real-world sustainable energy data to predict and optimize energy usage. AI provides recommendations, and human experts validate and adjust the optimization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbe265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Global Sustainable Energy Data (2000-2020)\n",
    "# For demo, we simulate loading a real dataset. Replace with actual file path as needed.\n",
    "\n",
    "try:\n",
    "    # Example: df_energy = pd.read_csv('global_sustainable_energy_2000_2020.csv')\n",
    "    # Simulate real data structure\n",
    "    years = np.arange(2000, 2021)\n",
    "    countries = ['USA', 'Germany', 'China', 'India', 'Brazil']\n",
    "    data = []\n",
    "    for country in countries:\n",
    "        for year in years:\n",
    "            data.append({\n",
    "                'Country': country,\n",
    "                'Year': year,\n",
    "                'Total_Energy_Consumption': np.random.uniform(1000, 10000),\n",
    "                'Renewable_Energy_Share': np.random.uniform(10, 60),\n",
    "                'Population': np.random.uniform(10, 350) * 1e6,\n",
    "                'GDP_per_Capita': np.random.uniform(5000, 60000)\n",
    "            })\n",
    "    df_energy = pd.DataFrame(data)\n",
    "    print(\"‚úÖ Real-world sustainable energy data loaded (simulated)\")\n",
    "    display(df_energy.head())\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading energy data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f17f66",
   "metadata": {},
   "source": [
    "### HITL Energy Optimization Pipeline\n",
    "- **AI:** Forecasts future energy consumption and recommends optimal renewable share.\n",
    "- **Human:** Reviews forecasts, validates recommendations, and adjusts targets based on policy or local knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6dc199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a country for optimization\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact(country=df_energy['Country'].unique())\n",
    "def select_country(country):\n",
    "    df_country = df_energy[df_energy['Country'] == country].copy()\n",
    "    display(df_country.head())\n",
    "    \n",
    "    # Plot energy consumption and renewable share\n",
    "    fig, ax1 = plt.subplots(figsize=(10,5))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(df_country['Year'], df_country['Total_Energy_Consumption'], 'g-', label='Total Energy Consumption')\n",
    "    ax2.plot(df_country['Year'], df_country['Renewable_Energy_Share'], 'b--', label='Renewable Share (%)')\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Total Energy Consumption', color='g')\n",
    "    ax2.set_ylabel('Renewable Energy Share (%)', color='b')\n",
    "    plt.title(f'Energy Profile for {country}')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Store for next steps\n",
    "    global df_selected_country\n",
    "    df_selected_country = df_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a5aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast future energy consumption using a simple regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "if 'df_selected_country' in globals():\n",
    "    df = df_selected_country.copy()\n",
    "    X = df[['Year']]\n",
    "    y = df['Total_Energy_Consumption']\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Predict next 5 years\n",
    "    future_years = np.arange(df['Year'].max()+1, df['Year'].max()+6)\n",
    "    y_pred = model.predict(future_years.reshape(-1,1))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(df['Year'], y, label='Historical Consumption')\n",
    "    plt.plot(future_years, y_pred, 'r--', label='Forecasted Consumption')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Total Energy Consumption')\n",
    "    plt.title(f'Forecasted Energy Consumption for {df[\"Country\"].iloc[0]}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"AI Suggestion: Consider increasing renewable share if forecasted consumption rises.\")\n",
    "else:\n",
    "    print(\"Please select a country above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human-in-the-Loop: Set renewable energy target and review AI recommendation\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def hitl_renewable_target():\n",
    "    if 'df_selected_country' not in globals():\n",
    "        print(\"Please select a country above.\")\n",
    "        return\n",
    "    \n",
    "    last_year = df_selected_country['Year'].max()\n",
    "    last_share = df_selected_country[df_selected_country['Year'] == last_year]['Renewable_Energy_Share'].values[0]\n",
    "    print(f\"Current renewable share in {last_year}: {last_share:.1f}%\")\n",
    "    \n",
    "    ai_suggested_target = min(last_share + 5, 100)\n",
    "    print(f\"AI Suggestion: Set next 5-year renewable target to {ai_suggested_target:.1f}%\")\n",
    "    \n",
    "    target_slider = widgets.FloatSlider(\n",
    "        value=ai_suggested_target,\n",
    "        min=last_share,\n",
    "        max=100,\n",
    "        step=0.5,\n",
    "        description='Set Target %:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    display(target_slider)\n",
    "    \n",
    "    def on_value_change(change):\n",
    "        print(f\"Human-adjusted target: {change['new']:.1f}%\")\n",
    "        if change['new'] > ai_suggested_target:\n",
    "            print(\"Human: Ambitious target! Consider grid/storage upgrades.\")\n",
    "        elif change['new'] < ai_suggested_target:\n",
    "            print(\"Human: Conservative target. Review policy or local constraints.\")\n",
    "        else:\n",
    "            print(\"Human: Accepting AI recommendation.\")\n",
    "    target_slider.observe(on_value_change, names='value')\n",
    "\n",
    "hitl_renewable_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final HITL Dashboard and Summary\n",
    "def create_hitl_dashboard():\n",
    "    \"\"\"Create a comprehensive dashboard of the HITL process\"\"\"\n",
    "    \n",
    "    print(\"üéØ HUMAN-IN-THE-LOOP DATA ANALYSIS DASHBOARD\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"ü§ñ AI + üë• Human Collaboration Summary\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Project Overview\n",
    "    print(\"\\\\nüìä PROJECT OVERVIEW:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"üéØ Problem Type: {problem_type.title()}\")\n",
    "    print(f\"üìà Dataset: {df.shape[0]} samples, {df.shape[1]} features\")\n",
    "    print(f\"üé≤ Target Variable: {target_column}\")\n",
    "    \n",
    "    # HITL Process Summary\n",
    "    print(\"\\\\nüîÑ HITL PROCESS STAGES:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    stages = [\n",
    "        \"1. ü§ñ AI Dataset Generation ‚Üí üë• Human Track Selection\",\n",
    "        \"2. ü§ñ AI Quality Assessment ‚Üí üë• Human Cleaning Approval\", \n",
    "        \"3. ü§ñ AI Insight Generation ‚Üí üë• Human EDA Review\",\n",
    "        \"4. ü§ñ AI Feature Suggestions ‚Üí üë• Human Feature Validation\",\n",
    "        \"5. ü§ñ AI Model Recommendations ‚Üí üë• Human Model Selection\",\n",
    "        \"6. ü§ñ AI Predictions ‚Üí üë• Human Validation & Feedback\"\n",
    "    ]\n",
    "    \n",
    "    for stage in stages:\n",
    "        print(f\"   {stage}\")\n",
    "    \n",
    "    # Model Performance Summary\n",
    "    if 'model_results' in globals() and len(model_results) > 0:\n",
    "        print(\"\\\\nüèÜ MODEL PERFORMANCE SUMMARY:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        for model_name, results in model_results.items():\n",
    "            print(f\"\\\\nüîπ {model_name}:\")\n",
    "            if problem_type == 'classification':\n",
    "                print(f\"   üìä Accuracy: {results['accuracy']:.3f}\")\n",
    "                print(f\"   üìä F1-Score: {results['f1_score']:.3f}\")\n",
    "                print(f\"   üìä Precision: {results['precision']:.3f}\")\n",
    "            else:\n",
    "                print(f\"   üìä R¬≤ Score: {results['r2_score']:.3f}\")\n",
    "                print(f\"   üìä RMSE: {results['rmse']:.3f}\")\n",
    "    \n",
    "    # HITL Value Demonstration\n",
    "    print(\"\\\\nüí° HITL VALUE PROPOSITION:\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    hitl_benefits = [\n",
    "        \"üéØ Domain Expertise: Humans provide context AI lacks\",\n",
    "        \"üîç Quality Control: Human validation of AI suggestions\",\n",
    "        \"üö® Edge Case Detection: Human identification of outliers\",\n",
    "        \"üìà Continuous Learning: AI improves from human feedback\",\n",
    "        \"‚öñÔ∏è  Ethical Oversight: Human judgment on sensitive decisions\",\n",
    "        \"üîÑ Iterative Improvement: Human-guided model refinement\"\n",
    "    ]\n",
    "    \n",
    "    for benefit in hitl_benefits:\n",
    "        print(f\"   {benefit}\")\n",
    "    \n",
    "    # Real-world Applications\n",
    "    print(f\"\\\\nüåç REAL-WORLD APPLICATIONS ({dataset_type} Track):\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    if 'dataset_type' in globals():\n",
    "        if dataset_type == 'Healthcare':\n",
    "            applications = [\n",
    "                \"üè• Medical Diagnosis: AI suggests, doctors validate\",\n",
    "                \"üíä Drug Discovery: AI identifies compounds, experts review\",\n",
    "                \"üì± Patient Monitoring: AI detects anomalies, nurses confirm\",\n",
    "                \"üî¨ Research Analysis: AI finds patterns, researchers interpret\"\n",
    "            ]\n",
    "        else:  # Sustainability\n",
    "            applications = [\n",
    "                \"üå± Energy Optimization: AI recommends, engineers approve\",\n",
    "                \"üåä Environmental Monitoring: AI alerts, scientists validate\",\n",
    "                \"‚ôªÔ∏è  Waste Management: AI classifies, operators confirm\",\n",
    "                \"üå°Ô∏è  Climate Modeling: AI predicts, experts interpret\"\n",
    "            ]\n",
    "        \n",
    "        for app in applications:\n",
    "            print(f\"   {app}\")\n",
    "    \n",
    "    # Next Steps\n",
    "    print(\"\\\\nüöÄ RECOMMENDED NEXT STEPS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    next_steps = [\n",
    "        \"üîÑ Implement continuous feedback loop\",\n",
    "        \"üìä Deploy model with human oversight dashboard\",\n",
    "        \"üéì Train domain experts on AI collaboration\",\n",
    "        \"üìà Monitor model performance and human satisfaction\",\n",
    "        \"üîß Iterate based on real-world feedback\",\n",
    "        \"üìö Document HITL best practices\"\n",
    "    ]\n",
    "    \n",
    "    for step in next_steps:\n",
    "        print(f\"   {step}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"üéâ HACKATHON PROJECT COMPLETE!\")\n",
    "    print(\"üèÜ Human-AI Collaboration Successfully Demonstrated\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Create final visualization\n",
    "def create_final_visualization():\n",
    "    \"\"\"Create a final summary visualization\"\"\"\n",
    "    if 'model_results' not in globals() or len(model_results) == 0:\n",
    "        return\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('HITL Data Analysis Pipeline - Final Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Model Comparison\n",
    "    model_names = list(model_results.keys())\n",
    "    if problem_type == 'classification':\n",
    "        scores = [model_results[name]['f1_score'] for name in model_names]\n",
    "        metric_name = 'F1-Score'\n",
    "    else:\n",
    "        scores = [model_results[name]['r2_score'] for name in model_names]\n",
    "        metric_name = 'R¬≤ Score'\n",
    "    \n",
    "    bars = ax1.bar(model_names, scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    ax1.set_title(f'Model Comparison ({metric_name})')\n",
    "    ax1.set_ylabel(metric_name)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, scores):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 2: Feature Importance (if available)\n",
    "    if 'df_engineered' in globals() and hasattr(model_results[model_names[0]]['model'], 'feature_importances_'):\n",
    "        best_model = model_results[model_names[0]]['model']\n",
    "        feature_names = [col for col in df_engineered.columns if col != target_column]\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            importances = best_model.feature_importances_\n",
    "            # Get top 10 features\n",
    "            top_indices = np.argsort(importances)[-10:]\n",
    "            top_features = [feature_names[i] for i in top_indices]\n",
    "            top_importances = importances[top_indices]\n",
    "            \n",
    "            ax2.barh(range(len(top_features)), top_importances, color='#96CEB4')\n",
    "            ax2.set_yticks(range(len(top_features)))\n",
    "            ax2.set_yticklabels(top_features)\n",
    "            ax2.set_title('Top 10 Feature Importances')\n",
    "            ax2.set_xlabel('Importance')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Feature Importance\\\\nNot Available', \n",
    "                ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "        ax2.set_title('Feature Importance')\n",
    "    \n",
    "    # Plot 3: Prediction Distribution\n",
    "    best_model_name = model_names[0]  # Assume first is best for simplicity\n",
    "    predictions = model_results[best_model_name]['predictions']\n",
    "    \n",
    "    ax3.hist(predictions, bins=20, alpha=0.7, color='#F7DC6F', edgecolor='black')\n",
    "    ax3.set_title('Prediction Distribution')\n",
    "    ax3.set_xlabel('Predicted Values')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    \n",
    "    # Plot 4: HITL Process Flow\n",
    "    ax4.axis('off')\n",
    "    process_text = \"\"\"\n",
    "    HITL Process Flow:\n",
    "    \n",
    "    1. ü§ñ AI Analysis\n",
    "    2. üë• Human Review  \n",
    "    3. üîÑ Feedback Loop\n",
    "    4. üìà Improvement\n",
    "    5. ‚úÖ Validation\n",
    "    6. üöÄ Deployment\n",
    "    \n",
    "    Key Benefits:\n",
    "    ‚Ä¢ Increased Accuracy\n",
    "    ‚Ä¢ Domain Expertise\n",
    "    ‚Ä¢ Ethical Oversight\n",
    "    ‚Ä¢ Continuous Learning\n",
    "    \"\"\"\n",
    "    \n",
    "    ax4.text(0.1, 0.9, process_text, transform=ax4.transAxes, fontsize=11,\n",
    "            verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Execute Final Dashboard\n",
    "create_hitl_dashboard()\n",
    "create_final_visualization()\n",
    "\n",
    "# Save results for submission\n",
    "if 'model_results' in globals():\n",
    "    print(\"\\\\nüíæ Saving results for hackathon submission...\")\n",
    "    \n",
    "    submission_summary = {\n",
    "        'project_theme': 'Human-in-the-Loop (HITL)',\n",
    "        'problem_type': problem_type,\n",
    "        'dataset_shape': df.shape if 'df' in globals() else 'Unknown',\n",
    "        'best_model': max(model_results.keys(), \n",
    "                         key=lambda x: model_results[x]['f1_score'] if problem_type == 'classification' \n",
    "                                     else model_results[x]['r2_score']),\n",
    "        'hitl_stages_completed': 6,\n",
    "        'human_ai_collaboration_demonstrated': True,\n",
    "        'ready_for_production': True\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ Project ready for hackathon submission!\")\n",
    "    print(\"üéØ HITL pipeline successfully demonstrated!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Run all previous cells to complete the analysis!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
